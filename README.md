# Synopsys Science Fair 2022
## Title: Voice-interactive kitchen helper robot for the disabled and visually impaired 
### Abstract
<p>The goal is to design and develop a prototype of a scalable, voice-interactive kitchen helper robot with electromagnetic arms for the visually impaired and handicapped. The system should have the capability to pick up spoons, forks, knives, and other common objects for the user. The system should have a voice recognition capability that converts the voice to text. Then it should be able to parse the text to determine the action, carry it out, and give a voice response back to the user.</p>
<p>The system is designed to parse and understand the spoken commands correctly using the Google Cloud Platform (GCP) speech-to-text API. Python code in the Nvidia Jetson Linux box gets the spoken voice command from the microphone, sends it to the GCP Speech-to-Text API for recognition. Once the text command is parsed and understood, the system then sends a command through the serial port to the Arduino Nano microcontroller to move the robot arm to pick up the object. Arduino Nano moves the robot arm to the appropriate place, lifts the object using an electromagnet, carries it to the user, then outputs a voice message using the eSpeak program.</p>
<p>It was found that Google Cloud Speech-to-Text API has good performance parsing English commands. Also, Uxcell’s 5V 50N electromagnet had better pick-and-place performance compared to Keyestudio’s 5V DC 25N electromagnet due to its higher electromagnetic induction strength. Further, the objects did not get dropped when the robot arm moved when Uxcell’s 5V 50N electromagnet was used.</p>
<img width="752" alt="image" src="https://user-images.githubusercontent.com/97870868/155863610-5d4e4e98-c58f-4202-86b5-05a5064eaf65.png">
<img width="746" alt="image" src="https://user-images.githubusercontent.com/97870868/155863632-41b6e3c2-1b0f-4513-a84d-a3244723564f.png">
